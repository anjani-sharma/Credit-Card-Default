{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Default Credit Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Business Context** - Banks are primarily known for money lending business. The more money they lend to people whom they can get good interest with timely repayment, the more revenue is for the banks.\n",
    "* **\n",
    "* The more banks are able to identify borrowers going towards serious delinquency rate, the better will be the bank's money lending business which in turn will lead to better revenue and better image in the market and with respect to competitiors. \n",
    "* **\n",
    "* * **Delinquent** in general is a slightly mild term where a borrower is not repaying charges and is behind by certain months whereas * **Default** is a term where a borrower has not been able to pay charges and is behind for a long period of months and is unlikely to repay the charges.\n",
    "* **\n",
    "* We have a general profile about the borrower such as age, Monthly Income, Dependents and the historical data such as what is the Debt Ratio, what ratio of amount is owed wrt credit limit, and the no of times defaulted in the past one, two, three months.\n",
    "* We will be using all these features to predict whether the borrower is likely to delinquent in the next 2 years or not.\n",
    "* These kind of predictions will help banks to take necessary actions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:53.449476Z",
     "start_time": "2021-04-10T02:13:53.446484Z"
    }
   },
   "outputs": [],
   "source": [
    "#* **Objective** : Building a model using the inputs/attributes which are general profile and historical records of a borrower to predict whether one is likely to have serious delinquency in the next 2 years "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T06:32:41.774657Z",
     "start_time": "2021-03-25T06:32:41.769670Z"
    }
   },
   "source": [
    "* **Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "# tested on python=3.8.10\n",
    "install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:54.108048Z",
     "start_time": "2021-04-10T02:13:54.105024Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing required libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# tested on python=3.8.10\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "# tested on python=3.8.10\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import kurtosis\n",
    "from scipy import stats, special\n",
    "from scipy.stats import skew\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:54.473038Z",
     "start_time": "2021-04-10T02:13:54.469051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:54.791188Z",
     "start_time": "2021-04-10T02:13:54.787200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display all rows and columns of a dataframe\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:55.067447Z",
     "start_time": "2021-04-10T02:13:55.060467Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing ML algorithms\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score,confusion_matrix, roc_curve, auc,classification_report, recall_score, precision_score, f1_score,roc_auc_score,auc,roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:55.342711Z",
     "start_time": "2021-04-10T02:13:55.338722Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-25T09:37:27.399167Z",
     "start_time": "2021-03-25T09:37:27.383177Z"
    }
   },
   "source": [
    "* ** Showing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:55.886295Z",
     "start_time": "2021-04-10T02:13:55.726684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the training dataset\n",
    "df = pd.read_csv('../input/cs-training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:56.129606Z",
     "start_time": "2021-04-10T02:13:56.125618Z"
    }
   },
   "outputs": [],
   "source": [
    "# No of rows and the columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:56.244298Z",
     "start_time": "2021-04-10T02:13:56.216373Z"
    }
   },
   "outputs": [],
   "source": [
    "# A general information about the datatype of an attribute and missing values(if any)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:56.493633Z",
     "start_time": "2021-04-10T02:13:56.476677Z"
    }
   },
   "outputs": [],
   "source": [
    "# what percentage of data is missing in the feature\n",
    "round(df.isnull().sum(axis=0)/len(df),2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:57.004265Z",
     "start_time": "2021-04-10T02:13:56.991298Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:57.028201Z",
     "start_time": "2021-04-10T02:13:57.006259Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the unique number of borrowers\n",
    "df['Unnamed: 0'].nunique()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:57.247613Z",
     "start_time": "2021-04-10T02:13:57.242627Z"
    }
   },
   "outputs": [],
   "source": [
    "df.rename(columns = {'Unnamed: 0' : 'CustomerID'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:57.282519Z",
     "start_time": "2021-04-10T02:13:57.274542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Target Variable\n",
    "print(df['SeriousDlqin2yrs'].unique())\n",
    "print()\n",
    "print('{}% of the borrowers falling in the serious delinquency '.format((df['SeriousDlqin2yrs'].sum()/len(df))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:57.622609Z",
     "start_time": "2021-04-10T02:13:57.467060Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "df['SeriousDlqin2yrs'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=axes[0])\n",
    "axes[0].set_title('SeriousDlqin2yrs')\n",
    "sns.countplot(x='SeriousDlqin2yrs',data=df,ax=axes[1])\n",
    "axes[1].set_title('SeriousDlqin2yrs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:58.071408Z",
     "start_time": "2021-04-10T02:13:58.063431Z"
    }
   },
   "outputs": [],
   "source": [
    "df['SeriousDlqin2yrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:58.486300Z",
     "start_time": "2021-04-10T02:13:58.371608Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating the dataset into train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **We will split the dataset into training(80%) on which modeling will be done using statistical/machine learning technqiues**\n",
    "* **\n",
    "* **Another one is the test dataset(20%) on which predictions will be made and check how the model is performing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:59.184461Z",
     "start_time": "2021-04-10T02:13:59.171465Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df.drop(columns = ['SeriousDlqin2yrs'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:59.195401Z",
     "start_time": "2021-04-10T02:13:59.185427Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:59.209391Z",
     "start_time": "2021-04-10T02:13:59.197428Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:59.545495Z",
     "start_time": "2021-04-10T02:13:59.435757Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into train and test validation\n",
    "df_test, df_train, y_test, y_train = train_test_split(data, y, test_size = 0.8, random_state=42, stratify = y)\n",
    "df_test.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:13:59.552445Z",
     "start_time": "2021-04-10T02:13:59.547458Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Event rate in the training dataset : ',np.mean(y_train))\n",
    "print()\n",
    "print('Event rate in the test dataset : ',np.mean(y_test))\n",
    "print()\n",
    "print('Event rate in the entire dataset : ',np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Event rate/Default Rate is maintainted in the training and test dataset after splitting in line with the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:00.145868Z",
     "start_time": "2021-04-10T02:14:00.127937Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.concat([df_train, y_train], axis=1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:05.577550Z",
     "start_time": "2021-04-10T02:14:05.568575Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.concat([df_test, y_test], axis=1)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Analysis using Training Numerical dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:06.015410Z",
     "start_time": "2021-04-10T02:14:06.011421Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_hist_boxplot(column):\n",
    "    fig,[ax1,ax2]=plt.subplots(1,2,figsize=(12,5))\n",
    "    sns.distplot(train[train[column].notnull()][column],ax=ax1)\n",
    "    sns.boxplot(y=train[train[column].notnull()][column],ax=ax2)\n",
    "    print(\"skewness : \",skew(train[train[column].notnull()][column]))\n",
    "    print(\"kurtosis : \",kurtosis(train[train[column].notnull()][column]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:06.404340Z",
     "start_time": "2021-04-10T02:14:06.400347Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_count_boxplot(column):\n",
    "    fig,[ax1,ax2]=plt.subplots(1,2,figsize=(12,6))\n",
    "    sns.countplot(train[train[column].notnull()][column],ax=ax1)\n",
    "    sns.boxplot(y=train[train[column].notnull()][column],ax=ax2)\n",
    "    print(\"skewness : \",skew(train[train[column].notnull()][column]))\n",
    "    print(\"kurtosis : \",kurtosis(train[train[column].notnull()][column]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:07.629060Z",
     "start_time": "2021-04-10T02:14:06.647685Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hist_boxplot('RevolvingUtilizationOfUnsecuredLines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:08.182578Z",
     "start_time": "2021-04-10T02:14:07.631054Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hist_boxplot('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:09.528974Z",
     "start_time": "2021-04-10T02:14:08.263399Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hist_boxplot('DebtRatio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:10.497384Z",
     "start_time": "2021-04-10T02:14:09.704505Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hist_boxplot('MonthlyIncome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:11.218453Z",
     "start_time": "2021-04-10T02:14:10.499378Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hist_boxplot('NumberOfOpenCreditLinesAndLoans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:13.078476Z",
     "start_time": "2021-04-10T02:14:12.121039Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hist_boxplot('NumberRealEstateLoansOrLines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:13.623020Z",
     "start_time": "2021-04-10T02:14:13.300882Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_count_boxplot('NumberOfDependents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:14.730089Z",
     "start_time": "2021-04-10T02:14:13.846422Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hist_boxplot('NumberOfTime30-59DaysPastDueNotWorse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:15.800223Z",
     "start_time": "2021-04-10T02:14:14.952462Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hist_boxplot('NumberOfTime60-89DaysPastDueNotWorse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:16.866370Z",
     "start_time": "2021-04-10T02:14:16.033567Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_hist_boxplot('NumberOfTimes90DaysLate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:17.118663Z",
     "start_time": "2021-04-10T02:14:17.115671Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_for_stats = ['RevolvingUtilizationOfUnsecuredLines', 'age',\n",
    "       'NumberOfTime30-59DaysPastDueNotWorse', 'DebtRatio', 'MonthlyIncome',\n",
    "       'NumberOfOpenCreditLinesAndLoans', 'NumberOfTimes90DaysLate',\n",
    "       'NumberRealEstateLoansOrLines', 'NumberOfTime60-89DaysPastDueNotWorse',\n",
    "       'NumberOfDependents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:17.625309Z",
     "start_time": "2021-04-10T02:14:17.366998Z"
    }
   },
   "outputs": [],
   "source": [
    "skewness  = [] ; kurt = []\n",
    "for column in cols_for_stats:\n",
    "    skewness.append(skew(train[train[column].notnull()][column]))\n",
    "    kurt.append(kurtosis(train[train[column].notnull()][column]))\n",
    "    \n",
    "stats = pd.DataFrame({'Skewness' : skewness, 'Kurtosis' : kurt},index=[col for col in cols_for_stats])\n",
    "stats.sort_values(by=['Skewness'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * Data distribution in the columns are highly right skewed with very high kurtosis value showing extreme outliers in those columns\n",
    "* * Except age which is little normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T14:15:18.214180Z",
     "start_time": "2021-03-26T14:15:18.207166Z"
    }
   },
   "source": [
    "#### Outlier Treatement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T14:33:13.673837Z",
     "start_time": "2021-03-26T14:33:13.669848Z"
    }
   },
   "source": [
    "* *From the above boxplot graphs we can observe:*\n",
    "* **\n",
    "* *In the columns NumberOfTime30-59DaysPastDueNotWorse , NumberOfTime60-89DaysPastDueNotWorse and NumberOfTimes90DaysLate, we see delinquency range beyond 90 which is common across all 3 features.*\n",
    "* **\n",
    "* *Treating outliers for the columns  -- NumberOfTime30-59DaysPastDueNotWorse , NumberOfTime60-89DaysPastDueNotWorse and NumberOfTimes90DaysLate*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:17.926501Z",
     "start_time": "2021-04-10T02:14:17.876634Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Unique values in '30-59 Days' values that are more than or equal to 90:\",np.unique(train[train['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n",
    "                                                                                          ['NumberOfTime30-59DaysPastDueNotWorse']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '60-89 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(train[train['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n",
    "                                                                                                       ['NumberOfTime60-89DaysPastDueNotWorse']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '90 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(train[train['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n",
    "                                                                                                    ['NumberOfTimes90DaysLate']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '30-59 Days' values that are less than 90:\",np.unique(train[train['NumberOfTime30-59DaysPastDueNotWorse']<90]\n",
    "                                                                                          ['NumberOfTime30-59DaysPastDueNotWorse']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '60-89 Days' when '30-59 Days' values are less than 90:\",np.unique(train[train['NumberOfTime30-59DaysPastDueNotWorse']<90]\n",
    "                                                                                           ['NumberOfTime60-89DaysPastDueNotWorse']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '90 Days' when '30-59 Days' values are less than 90:\",np.unique(train[train['NumberOfTime30-59DaysPastDueNotWorse']<90]\n",
    "                                                                                        ['NumberOfTimes90DaysLate']))\n",
    "\n",
    "\n",
    "print(\"Proportion of positive class with special 96/98 values:\",\n",
    "      round(train[train['NumberOfTime30-59DaysPastDueNotWorse']>=90]['SeriousDlqin2yrs'].sum()*100/\n",
    "      len(train[train['NumberOfTime30-59DaysPastDueNotWorse']>=90]['SeriousDlqin2yrs']),2),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *We can see from the following that when records in column 'NumberOfTime30-59DaysPastDueNotWorse' are more than 90, the other columns that records number of times payments are past due X days also have the same values. We will classify these as special labels since the proportion of positive class is abnormally high at 55.56%.*\n",
    "* **\n",
    "* *These 96 and 98 values can be viewed as accounting errors. Hence, we would replace them with the maximum value before 96 i.e. 12, 11 and 17*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:18.177861Z",
     "start_time": "2021-04-10T02:14:18.169851Z"
    }
   },
   "outputs": [],
   "source": [
    "train.loc[train['NumberOfTime30-59DaysPastDueNotWorse'] >= 90, 'NumberOfTime30-59DaysPastDueNotWorse'] = 12\n",
    "train.loc[train['NumberOfTime60-89DaysPastDueNotWorse'] >= 90, 'NumberOfTime60-89DaysPastDueNotWorse'] = 11\n",
    "train.loc[train['NumberOfTimes90DaysLate'] >= 90, 'NumberOfTimes90DaysLate'] = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:18.441123Z",
     "start_time": "2021-04-10T02:14:18.430154Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Unique values in 30-59Days\", np.unique(train['NumberOfTime30-59DaysPastDueNotWorse']))\n",
    "print(\"Unique values in 60-89Days\", np.unique(train['NumberOfTime60-89DaysPastDueNotWorse']))\n",
    "print(\"Unique values in 90Days\", np.unique(train['NumberOfTimes90DaysLate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:18.473039Z",
     "start_time": "2021-04-10T02:14:18.442120Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Unique values in '30-59 Days' values that are more than or equal to 90:\",np.unique(test[test['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n",
    "                                                                                          ['NumberOfTime30-59DaysPastDueNotWorse']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '60-89 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(test[test['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n",
    "                                                                                                       ['NumberOfTime60-89DaysPastDueNotWorse']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '90 Days' when '30-59 Days' values are more than or equal to 90:\",np.unique(test[test['NumberOfTime30-59DaysPastDueNotWorse']>=90]\n",
    "                                                                                                    ['NumberOfTimes90DaysLate']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '30-59 Days' values that are less than 90:\",np.unique(test[test['NumberOfTime30-59DaysPastDueNotWorse']<90]\n",
    "                                                                                          ['NumberOfTime30-59DaysPastDueNotWorse']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '60-89 Days' when '30-59 Days' values are less than 90:\",np.unique(test[test['NumberOfTime30-59DaysPastDueNotWorse']<90]\n",
    "                                                                                           ['NumberOfTime60-89DaysPastDueNotWorse']))\n",
    "\n",
    "\n",
    "print(\"Unique values in '90 Days' when '30-59 Days' values are less than 90:\",np.unique(test[test['NumberOfTime30-59DaysPastDueNotWorse']<90]\n",
    "                                                                                        ['NumberOfTimes90DaysLate']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T14:52:22.607737Z",
     "start_time": "2021-03-26T14:52:22.599709Z"
    }
   },
   "source": [
    "* *Since, these values exist in Test Set as well. Therefore, replacing them with maximum values before 96 and 98 i.e. 13, 7 and 15.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:18.950760Z",
     "start_time": "2021-04-10T02:14:18.936797Z"
    }
   },
   "outputs": [],
   "source": [
    "test.loc[test['NumberOfTime30-59DaysPastDueNotWorse'] >= 90, 'NumberOfTime30-59DaysPastDueNotWorse'] = 13\n",
    "test.loc[test['NumberOfTime60-89DaysPastDueNotWorse'] >= 90, 'NumberOfTime60-89DaysPastDueNotWorse'] = 7\n",
    "test.loc[test['NumberOfTimes90DaysLate'] >= 90, 'NumberOfTimes90DaysLate'] = 15\n",
    "print(\"Unique values in 30-59Days\", np.unique(test['NumberOfTime30-59DaysPastDueNotWorse']))\n",
    "print(\"Unique values in 60-89Days\", np.unique(test['NumberOfTime60-89DaysPastDueNotWorse']))\n",
    "print(\"Unique values in 90Days\", np.unique(test['NumberOfTimes90DaysLate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T15:07:42.822314Z",
     "start_time": "2021-03-26T15:07:42.814318Z"
    }
   },
   "source": [
    " * *Checking for * **DebtRatio** and * **RevolvingUtilizationOfUnsecuredLines.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:19.209069Z",
     "start_time": "2021-04-10T02:14:19.189123Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Debt Ratio: \\n',train['DebtRatio'].describe())\n",
    "print('\\nRevolving Utilization of Unsecured Lines: \\n',train['RevolvingUtilizationOfUnsecuredLines'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T15:10:26.991590Z",
     "start_time": "2021-03-26T15:10:26.983561Z"
    }
   },
   "source": [
    "* **Debt Ratio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see data is right skewed. So, our main aim would be to check the potential outliers beyond 95% quantiles. However, since our data is 120,000, let's consider 95% and 97.5% quantiles for our further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:19.469374Z",
     "start_time": "2021-04-10T02:14:19.446434Z"
    }
   },
   "outputs": [],
   "source": [
    "train[train['DebtRatio'] >= train['DebtRatio'].quantile(0.95)][['SeriousDlqin2yrs','MonthlyIncome']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we can observe:\n",
    "* **\n",
    "* Out of 6002 customers falling in the last 5 percentile of the data i.e. the number of times their debt is higher than their income, only 308 have Monthly Income values.\n",
    "* The Max for Monthly Income is 1 and Min is 0 which makes us wonder that are data entry errors. Let's check whether the Serious Delinquency in 2 years and Monthly Income values are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:19.717709Z",
     "start_time": "2021-04-10T02:14:19.707735Z"
    }
   },
   "outputs": [],
   "source": [
    "train[(train[\"DebtRatio\"] > train[\"DebtRatio\"].quantile(0.95)) & (train['SeriousDlqin2yrs'] == train['MonthlyIncome'])].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hence, our suspects are true and there are 271 out of 308 rows where Monthly Income is equal to the Serious Delinquencies in 2 years. Hence we will remove these 271 outliers from our analysis as their current values aren't useful for our predictive modelling and will add to the bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:19.979010Z",
     "start_time": "2021-04-10T02:14:19.957068Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train = train[-((train[\"DebtRatio\"] > train[\"DebtRatio\"].quantile(0.95)) & (train['SeriousDlqin2yrs'] == train['MonthlyIncome']))]\n",
    "new_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Revolving Utilization of Unsecured Lines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This field basically represents the ratio of the amount owed by the credit limit of a customer. A ratio higher than 1 is considered to be a serious defaulter. A Ratio of 10 functionally also seems possible, let's see how many of these customers have the Revolving Utilization of Unsecured Lines greater than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:21.370867Z",
     "start_time": "2021-04-10T02:14:21.330976Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train[new_train['RevolvingUtilizationOfUnsecuredLines']>10].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T16:57:38.378125Z",
     "start_time": "2021-03-26T16:57:38.370098Z"
    }
   },
   "source": [
    "* Despite owing thousands, out of these these 187 people ,less than 10 people are falling in the serious delinquency which means this might be another error. Even if it is not an error, these numbers will add huge bias and variance to our final predictions. Therefore, the best decision is to remove these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:23.658221Z",
     "start_time": "2021-04-10T02:14:23.640268Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train = new_train[new_train['RevolvingUtilizationOfUnsecuredLines']<=10]\n",
    "new_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:24.149905Z",
     "start_time": "2021-04-10T02:14:24.139932Z"
    }
   },
   "outputs": [],
   "source": [
    "new_test = test[test['RevolvingUtilizationOfUnsecuredLines']<=10]\n",
    "new_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:26.479237Z",
     "start_time": "2021-04-10T02:14:26.454303Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train[['age', 'SeriousDlqin2yrs']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It can be observed that the data includes a record with age = 0 which is not a valid age ,updating the record with mode age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:27.071656Z",
     "start_time": "2021-04-10T02:14:27.054697Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train[new_train['age']<1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will be replacing the error/odd value with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:28.336954Z",
     "start_time": "2021-04-10T02:14:28.327014Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train.loc[new_train['age'] == 0, 'age'] = new_train.age.mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:29.366931Z",
     "start_time": "2021-04-10T02:14:29.361976Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train['age'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:31.574920Z",
     "start_time": "2021-04-10T02:14:31.565944Z"
    }
   },
   "outputs": [],
   "source": [
    "new_test[new_test['age']<1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* No such discrepancy is found in the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Value Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T14:11:39.127254Z",
     "start_time": "2021-03-26T14:11:39.123269Z"
    }
   },
   "source": [
    "* *Since MonthlyIncome is an integer value, we will replace the nulls with the median values instead of mean as it was heavily right skewed.*\n",
    "* **\n",
    "* *Number of Dependents can be characterized as a categorical variable, hence if customers have NA for number of dependents, it means that they do not have any dependents. \n",
    "Filling either by mode which is 0 or by the above assumption is the same.Therefore, we fill them with zeros.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:32.345128Z",
     "start_time": "2021-04-10T02:14:32.340110Z"
    }
   },
   "outputs": [],
   "source": [
    "def MissingHandler(df):\n",
    "    \n",
    "    DataMissing = df.isnull().sum()*100/len(df)\n",
    "    DataMissingByColumn = pd.DataFrame({'Percentage Nulls':DataMissing})\n",
    "    DataMissingByColumn.sort_values(by='Percentage Nulls',ascending=False,inplace=True)\n",
    "    return DataMissingByColumn[DataMissingByColumn['Percentage Nulls']>0]\n",
    "\n",
    "#MissingHandler(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:32.497688Z",
     "start_time": "2021-04-10T02:14:32.490706Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train['MonthlyIncome'].fillna(new_train['MonthlyIncome'].median(), inplace=True)\n",
    "new_train['NumberOfDependents'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T14:27:44.394452Z",
     "start_time": "2021-03-26T14:27:44.389467Z"
    }
   },
   "source": [
    "* **Filling missing values with the same logic on the validation and the test dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:33.020322Z",
     "start_time": "2021-04-10T02:14:33.013309Z"
    }
   },
   "outputs": [],
   "source": [
    "new_test['MonthlyIncome'].fillna(new_test['MonthlyIncome'].median(), inplace=True)\n",
    "new_test['NumberOfDependents'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T14:21:31.477289Z",
     "start_time": "2021-03-26T14:21:31.472308Z"
    }
   },
   "source": [
    "* ** Rechecking Nulls**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:33.568853Z",
     "start_time": "2021-04-10T02:14:33.553862Z"
    }
   },
   "outputs": [],
   "source": [
    "MissingHandler(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:34.057551Z",
     "start_time": "2021-04-10T02:14:34.046543Z"
    }
   },
   "outputs": [],
   "source": [
    "MissingHandler(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:14:34.955061Z",
     "start_time": "2021-04-10T02:14:34.945054Z"
    }
   },
   "outputs": [],
   "source": [
    "MissingHandler(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Correlation Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:56:42.094164Z",
     "start_time": "2021-04-02T16:56:41.403721Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(new_train.corr(), annot=True, cmap=plt.cm.CMRmap_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From the correlation heatmap above, we can see the most correlated values to SeriousDlqin2yrs are NumberOfTime30-59DaysPastDueNotWorse , NumberOfTime60-89DaysPastDueNotWorse and NumberOfTimes90DaysLate.\n",
    "* **\n",
    "* Number of Open Credit Lines and Loans and Number of Real Estate Loans or Lines also have a significant correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:56:42.100148Z",
     "start_time": "2021-04-02T16:56:42.095161Z"
    }
   },
   "outputs": [],
   "source": [
    "def boxplot_violinplot(col1, col2):\n",
    "    fig,[ax1,ax2]=plt.subplots(1,2,figsize=(12,5))\n",
    "    sns.boxplot(x=col1, y=col2, data=new_train, palette='Set3',ax=ax1)\n",
    "    sns.violinplot(x=col1, y=col2, data=new_train, palette='Set3',ax=ax2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:56:42.634755Z",
     "start_time": "2021-04-02T16:56:42.102142Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs', 'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:56:42.639743Z",
     "start_time": "2021-04-02T16:56:42.636751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Age has no significant on the target variable though most of the customers are in the mid 50 who has serious delinquency in 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:56:43.516372Z",
     "start_time": "2021-04-02T16:56:42.641737Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs', 'MonthlyIncome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:56:43.521359Z",
     "start_time": "2021-04-02T16:56:43.517369Z"
    }
   },
   "outputs": [],
   "source": [
    "# More or less similar relationship when target is 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:57:22.924491Z",
     "start_time": "2021-04-02T16:57:21.787524Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs','DebtRatio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:57:22.930506Z",
     "start_time": "2021-04-02T16:57:22.927483Z"
    }
   },
   "outputs": [],
   "source": [
    "# More or less similar relationship when target is 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:57:25.163548Z",
     "start_time": "2021-04-02T16:57:24.526218Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs', 'NumberOfOpenCreditLinesAndLoans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is expected that the higher the utlization ratio, the higher the default rate is. Let me look into that by plotting the utlization ratio to default rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:57:43.707462Z",
     "start_time": "2021-04-02T16:57:42.930498Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs', 'NumberRealEstateLoansOrLines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:57:44.489337Z",
     "start_time": "2021-04-02T16:57:43.708421Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs', 'RevolvingUtilizationOfUnsecuredLines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:57:57.471452Z",
     "start_time": "2021-04-02T16:57:57.467463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature description itself more the utlization of the credit resources or the ratio of amount owe to credit is more, serious delinquency is on a bit higher side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:58:05.359656Z",
     "start_time": "2021-04-02T16:58:04.487982Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs', 'NumberOfDependents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:58:06.416869Z",
     "start_time": "2021-04-02T16:58:05.361652Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs', 'NumberOfTime30-59DaysPastDueNotWorse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:58:07.612679Z",
     "start_time": "2021-04-02T16:58:06.419829Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs', 'NumberOfTime60-89DaysPastDueNotWorse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T16:58:32.268890Z",
     "start_time": "2021-04-02T16:58:31.353310Z"
    }
   },
   "outputs": [],
   "source": [
    "boxplot_violinplot('SeriousDlqin2yrs', 'NumberOfTimes90DaysLate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is expected that the higher the utlization ratio, the higher the default rate is. Let me look into that by plotting the utlization ratio to default rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now let's move to the Feature Engineering section of our Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:37:05.880920Z",
     "start_time": "2021-04-03T00:37:05.872940Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = [new_train, new_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Making combined features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:37:12.777713Z",
     "start_time": "2021-04-03T00:37:12.756764Z"
    }
   },
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "    \n",
    "    data['CombinedPastDue']     = data['NumberOfTime30-59DaysPastDueNotWorse'] + data['NumberOfTime60-89DaysPastDueNotWorse'] + data['NumberOfTimes90DaysLate']\n",
    "    data['CombinedCreditLoans'] = data['NumberOfOpenCreditLinesAndLoans'] + data['NumberRealEstateLoansOrLines']\n",
    "    \n",
    "new_train.columns    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Interaction of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:37:19.692497Z",
     "start_time": "2021-04-03T00:37:19.662546Z"
    }
   },
   "outputs": [],
   "source": [
    "for data in dataset:\n",
    "\n",
    "    data['MonthlyIncomePerPerson'] = data['MonthlyIncome']/(data['NumberOfDependents']+1)\n",
    "    \n",
    "    data['MonthlyDebt']            = data['MonthlyIncome']*data['DebtRatio']\n",
    "    \n",
    "    data['isRetired']              = np.where((data['age'] > 65), 1, 0)\n",
    "    \n",
    "    data['RevolvingLines']         = data['NumberOfOpenCreditLinesAndLoans']-data['NumberRealEstateLoansOrLines']\n",
    "    data['hasRevolvingLines']      = np.where((data['RevolvingLines']>0),1,0)\n",
    "    \n",
    "    data['hasMultipleRealEstates'] = np.where((data['NumberRealEstateLoansOrLines']>=2),1,0)\n",
    "    \n",
    "    data['IsAlone']                = np.where((data['NumberOfDependents']==0),1,0)\n",
    "    \n",
    "new_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:37:27.070473Z",
     "start_time": "2021-04-03T00:37:27.065487Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train.shape, new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:37:34.553342Z",
     "start_time": "2021-04-03T00:37:34.547357Z"
    }
   },
   "outputs": [],
   "source": [
    "print(new_train['SeriousDlqin2yrs'].sum()/len(new_train))\n",
    "print()\n",
    "print(new_test['SeriousDlqin2yrs'].sum()/len(new_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Tackling Class Imbalance Problem using:**\n",
    "* **\n",
    "    * Upsampling the minority class(default rate)\n",
    "    * Downsampling the majority class(non defaulters)\n",
    "    * SMOTE - synthethic sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:37:42.490845Z",
     "start_time": "2021-04-03T00:37:42.442974Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = new_train.drop(columns=['CustomerID', 'SeriousDlqin2yrs'],axis=1)\n",
    "y_train = new_train['SeriousDlqin2yrs']\n",
    "\n",
    "df_test = new_test.drop(columns=['CustomerID', 'SeriousDlqin2yrs'],axis=1)\n",
    "y_test = new_test['SeriousDlqin2yrs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:37:50.021940Z",
     "start_time": "2021-04-03T00:37:49.999004Z"
    }
   },
   "outputs": [],
   "source": [
    "df_majority = new_train[new_train['SeriousDlqin2yrs']==0]\n",
    "df_minority = new_train[new_train['SeriousDlqin2yrs']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:37:57.967208Z",
     "start_time": "2021-04-03T00:37:57.898357Z"
    }
   },
   "outputs": [],
   "source": [
    "# replacing the samples keeping 100000 as the defaulters to keep in line with the non defaulters\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=100000, random_state=42)\n",
    "df_upsampled = pd.concat([df_majority,df_minority_upsampled])\n",
    "df_upsampled['SeriousDlqin2yrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:38:05.774833Z",
     "start_time": "2021-04-03T00:38:05.705018Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_upsampled = df_upsampled['SeriousDlqin2yrs']\n",
    "\n",
    "df_upsampled.drop(columns=['CustomerID', 'SeriousDlqin2yrs'],axis=1, inplace=True)\n",
    "\n",
    "df_upsampled.shape, df_test.shape, y_train_upsampled.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:38:14.968998Z",
     "start_time": "2021-04-03T00:38:14.960022Z"
    }
   },
   "outputs": [],
   "source": [
    "new_train['SeriousDlqin2yrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:38:23.221255Z",
     "start_time": "2021-04-03T00:38:23.203305Z"
    }
   },
   "outputs": [],
   "source": [
    "# keeping 8000 as  non defaulters to keep in line with the defaulters\n",
    "df_majority_downsampled = resample(df_majority, n_samples=8000, random_state=42)\n",
    "df_downsampled = pd.concat([df_minority,df_majority_downsampled])\n",
    "df_downsampled['SeriousDlqin2yrs'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:38:31.873213Z",
     "start_time": "2021-04-03T00:38:31.859251Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train_downsampled = df_downsampled['SeriousDlqin2yrs']\n",
    "\n",
    "df_downsampled.drop(columns=['CustomerID', 'SeriousDlqin2yrs'],axis=1, inplace=True)\n",
    "\n",
    "df_downsampled.shape, df_test.shape, y_train_downsampled.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:38:42.860105Z",
     "start_time": "2021-04-03T00:38:40.677852Z"
    }
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy = 'minority',k_neighbors = 2,random_state=42)\n",
    "\n",
    "os_data_X,os_data_y=smote.fit_resample(df_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:38:51.319151Z",
     "start_time": "2021-04-03T00:38:51.294220Z"
    }
   },
   "outputs": [],
   "source": [
    "os_data_X.shape, sum(os_data_y)/len(os_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now the event rate in the training dataset is 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:38:59.868760Z",
     "start_time": "2021-04-03T00:38:59.863774Z"
    }
   },
   "outputs": [],
   "source": [
    "# Making a copy of the dataframes so that tarnsformed df and original df is separate to comapre results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:39:08.516037Z",
     "start_time": "2021-04-03T00:39:08.495092Z"
    }
   },
   "outputs": [],
   "source": [
    "os_data_X_tranformed  = os_data_X.copy()\n",
    "df_test_transformed   = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:39:16.523989Z",
     "start_time": "2021-04-03T00:39:16.518005Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test_standaradized = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:39:24.568029Z",
     "start_time": "2021-04-03T00:39:24.539106Z"
    }
   },
   "outputs": [],
   "source": [
    "df_downsampled_transformed = df_downsampled.copy()\n",
    "df_upsampled_transformed   = df_upsampled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T06:27:23.878208Z",
     "start_time": "2021-03-30T06:27:23.873217Z"
    }
   },
   "source": [
    "* ** Scaling of the features so convergence of the model towards global minima can be achieved and training can be done faster**\n",
    "* We will be using a couple of Feature Transformation techniques:\n",
    "* **\n",
    "    * BoxCox Transformations\n",
    "    * Standaradization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T17:00:51.343473Z",
     "start_time": "2021-04-02T17:00:51.339449Z"
    }
   },
   "outputs": [],
   "source": [
    "# We will check with how skewness changes after the transformations and we will check on SMOTE sampling technique dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling features using BoxCox Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T17:00:51.375355Z",
     "start_time": "2021-04-02T17:00:51.345433Z"
    }
   },
   "outputs": [],
   "source": [
    "def SkewMeasure(df):\n",
    "    nonObjectColList = df.dtypes[df.dtypes != 'object'].index\n",
    "    skewM = df[nonObjectColList].apply(lambda x: skew(x.dropna())).sort_values(ascending = False)\n",
    "    skewM=pd.DataFrame({'skew':skewM})\n",
    "    return skewM[abs(skewM)>0.5].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:39:32.400214Z",
     "start_time": "2021-04-03T00:39:32.191910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SkewMeasure(os_data_X_tranformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:39:41.210014Z",
     "start_time": "2021-04-03T00:39:40.559716Z"
    }
   },
   "outputs": [],
   "source": [
    "skewM = SkewMeasure(os_data_X_tranformed)\n",
    "\n",
    "for i in skewM.index:\n",
    "    os_data_X_tranformed[i] = special.boxcox1p(os_data_X_tranformed[i],0.15) #lambda = 0.15\n",
    "    df_test_transformed[i]  = special.boxcox1p(df_test_transformed[i],0.15) #lambda = 0.15\n",
    "SkewMeasure(os_data_X_tranformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T17:01:40.213738Z",
     "start_time": "2021-04-02T17:01:21.690063Z"
    }
   },
   "outputs": [],
   "source": [
    "columnList = list(df_test_transformed.columns)\n",
    "\n",
    "fig = plt.figure(figsize=[20,20])\n",
    "for col,i in zip(columnList,range(1,19)):\n",
    "    axes = fig.add_subplot(6,3,i)\n",
    "    sns.distplot(os_data_X_tranformed[col],ax=axes, kde_kws={'bw':1.5}, color='purple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Skewness is reduced and now the graphs are looking good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:39:54.318515Z",
     "start_time": "2021-04-03T00:39:54.151950Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_transformed = df_train.copy()\n",
    "\n",
    "skewM = SkewMeasure(df_train)\n",
    "\n",
    "for i in skewM.index:\n",
    "    df_train_transformed[i] = special.boxcox1p(df_train_transformed[i],0.15) #lambda = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standaradization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:40:01.782546Z",
     "start_time": "2021-04-03T00:40:01.592983Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(os_data_X)\n",
    "\n",
    "X_train_scaled = scaler.transform(os_data_X) \n",
    "X_test_scaled = scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:40:09.299869Z",
     "start_time": "2021-04-03T00:40:09.125337Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_upsampled_transformed)\n",
    "\n",
    "X_train_scaled_upsampled = scaler.transform(df_upsampled_transformed) \n",
    "X_test_scaled_upsampled = scaler.transform(df_test_standaradized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:40:16.771678Z",
     "start_time": "2021-04-03T00:40:16.740730Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_downsampled_transformed)\n",
    "\n",
    "X_train_scaled_downsampled = scaler.transform(df_downsampled_transformed) \n",
    "X_test_scaled_downsampled = scaler.transform(df_test_standaradized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Training the model :**\n",
    "    * **\n",
    "    * We will be training the model using different algorithms \n",
    "    * We wil also be comparing the results using transformed dataset and the original dataset\n",
    "    * We will check and compare on which algorithms transformations works and on which not.\n",
    "    * We will be doing Cross Validation and will see if the model performance is fluctuating using the statistical measures such as mean and standarad deviation \n",
    "* **\n",
    "* ** Predicting the customers from the test dataset and comapring with the true values from the test dataset using different evaluation metrics will give us which model to go for and which model is performing the best**\n",
    "* **\n",
    "* ** We will also be evaluating our model on different metrics and why some metrics might fail in our case study which is one of the most popular metric to evaluate a model performance **\n",
    "* **\n",
    "* ** Some common evaluation metric you will be seeing across all modeling algorithms are:**\n",
    "    * ** Precision** - * How good a model is in predicting the events\n",
    "        * True Positive/(True Positive + False Positives)\n",
    "    * ** Recall** - * How good a model is able to capture relevant events(Coverage)\n",
    "        * True Positive/(True Positive + False Negatives)\n",
    "    * ** F1 Score** - * Harmonic mean between Precision and Recall, this metric penalized either one of the lower scorer metric so that trade off can be maintained\n",
    "    * ** Area Under the Curve & ROC** - * how good a model is able to differentiate between the classes\n",
    "        * It is plotted as False Positive Rate on the x-axis and True Positive Rate on the y-axis  using different thresholds.\n",
    "        * The more the area or the line it is above straight diagonal line, the better is model performance as comapred to randomly predicting the classes\n",
    "* **\n",
    "* ** We will be comapring the different modeling techniques AUC results to see which ones are performing better as compared to others**\n",
    "* **\n",
    "* ** We will be tuning hyper parameters of the models to achieve better results and see how tuning increases performance if it does**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T17:02:18.596696Z",
     "start_time": "2021-04-02T17:02:18.591722Z"
    }
   },
   "outputs": [],
   "source": [
    "# custom metrics\n",
    "def precision(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def recall(y_true, y_pred): \n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:44:08.096801Z",
     "start_time": "2021-04-03T00:44:08.090850Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_model(data, labels, test_data,test_label, epochs, batch_size):\n",
    "    \n",
    "    n_inputs = data.shape[1]\n",
    "    model    = keras.Sequential()\n",
    "    model.add(layers.Dense(16, activation ='relu', input_shape =(n_inputs, )))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(32,activation = 'relu'))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(1,activation ='sigmoid'))\n",
    "    \n",
    "    model_file_name = 'MLP_predict_default_case_study.hdf5'\n",
    "    ckpt = ModelCheckpoint(model_file_name, monitor='val_precision',\n",
    "                           save_best_only=True, mode='max')\n",
    "    early = EarlyStopping(monitor=\"val_recall\", mode=\"max\", patience=15)\n",
    "\n",
    "    model.compile(optimizer = 'adam',\n",
    "                 loss= 'binary_crossentropy',\n",
    "                 metrics = [precision,recall])\n",
    "    \n",
    "    history = model.fit(data,\n",
    "                       labels,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       callbacks=[ckpt, early],\n",
    "                       validation_data=(test_data,test_label))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:45:27.076453Z",
     "start_time": "2021-04-03T00:45:27.068474Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_precisions_thresolds(data, labels, test_data, test_label, epochs, batch_size):\n",
    "    trained_model = fit_model(data, labels, test_data, test_label, epochs=epochs, batch_size=batch_size)\n",
    "    y_test_pred   = trained_model.predict(test_data)\n",
    "    \n",
    "    P_macro = [] ; P_micro = [];  R_macro = [] ;R_micro = []; F1_macro = [] ;F1_micro = []; cut_off = [] ; metrics = pd.DataFrame()\n",
    "    threshold_list = [0.25,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.95,0.96]\n",
    "    for thres in threshold_list:\n",
    "        cut_off.append(thres)\n",
    "        y_test_pred_new = [1 if el>thres else 0 for el in y_test_pred]\n",
    "        prec_macro            = round(precision_score(test_label, y_test_pred_new, pos_label=1, average='macro'),2)\n",
    "        P_macro.append(prec_macro)\n",
    "        prec_micro            = round(precision_score(test_label, y_test_pred_new, pos_label=1, average='micro'),2)\n",
    "        P_micro.append(prec_micro)\n",
    "        rec_macro             = round(recall_score(test_label, y_test_pred_new, pos_label=1,average='macro'),2)\n",
    "        R_macro.append(rec_macro)\n",
    "        rec_micro             = round(recall_score(test_label, y_test_pred_new, pos_label=1,average='micro'),2)\n",
    "        R_micro.append(rec_micro)\n",
    "        f1_macro              = round(f1_score(test_label, y_test_pred_new, average='macro'),2)\n",
    "        F1_macro.append(f1_macro)\n",
    "        f1_micro              = round(f1_score(test_label, y_test_pred_new, average='micro'),2)\n",
    "        F1_micro.append(f1_micro)\n",
    "        \n",
    "    metrics = pd.DataFrame({'Threshold' : cut_off, 'Precision Macro' : P_macro, 'Precision Micro' : P_micro,'Recall Macro' : R_macro, 'Recall Micro' : R_micro,'F1 Score Macro' : F1_macro, 'F1 Score Micro' : F1_micro})\n",
    "    return metrics.sort_values(by=['Threshold'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Appling Neural Network model on Box Cox transformed dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:46:25.730891Z",
     "start_time": "2021-04-03T00:45:34.603014Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "box_cox_metrics = compute_precisions_thresolds(os_data_X_tranformed, os_data_y, df_test_transformed, y_test,epochs=15, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:46:32.723307Z",
     "start_time": "2021-04-03T00:46:32.710342Z"
    }
   },
   "outputs": [],
   "source": [
    "box_cox_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Applying Neural Network on Standardized dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:47:34.191207Z",
     "start_time": "2021-04-03T00:46:39.725542Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "standardized_metrics = compute_precisions_thresolds(X_train_scaled, os_data_y, X_test_scaled, y_test,epochs=15, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:47:41.366628Z",
     "start_time": "2021-04-03T00:47:41.354626Z"
    }
   },
   "outputs": [],
   "source": [
    "standardized_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T05:04:18.134308Z",
     "start_time": "2021-03-30T05:04:18.046545Z"
    }
   },
   "source": [
    "* ** Applying Neural Network on Original Standaradized dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:47:48.553999Z",
     "start_time": "2021-04-03T00:47:48.444260Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(df_train)\n",
    "\n",
    "df_train_scaled = scaler.transform(df_train) \n",
    "df_test_scaled = scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:48:18.915643Z",
     "start_time": "2021-04-03T00:47:55.553154Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_metrics = compute_precisions_thresolds(df_train_scaled, y_train, df_test_scaled, y_test, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:48:27.364548Z",
     "start_time": "2021-04-03T00:48:27.335624Z"
    }
   },
   "outputs": [],
   "source": [
    "base_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Applying Neural Network on Upsampled dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:49:11.398982Z",
     "start_time": "2021-04-03T00:48:34.666772Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upsampled_metrics = compute_precisions_thresolds(X_train_scaled_upsampled, y_train_upsampled, df_test_standaradized, y_test, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:49:11.414655Z",
     "start_time": "2021-04-03T00:49:11.398982Z"
    }
   },
   "outputs": [],
   "source": [
    "upsampled_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T09:11:31.264736Z",
     "start_time": "2021-03-30T09:11:31.256697Z"
    }
   },
   "source": [
    "* **Applying Neural Network on Downsampled dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:49:34.499362Z",
     "start_time": "2021-04-03T00:49:25.935092Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "downsampled_metrics = compute_precisions_thresolds(X_train_scaled_downsampled, y_train_downsampled, df_test_standaradized, y_test, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:49:41.590222Z",
     "start_time": "2021-04-03T00:49:41.576258Z"
    }
   },
   "outputs": [],
   "source": [
    "downsampled_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T08:39:21.721909Z",
     "start_time": "2021-03-30T08:39:21.713916Z"
    }
   },
   "source": [
    "* It can be cleary seen that Deep learning models require scaling of the features so that error can be minimised when features are nearly of the same scale so that bakcpropogation of errors can be there which results in better optimization\n",
    "* **\n",
    "* No effect on Neural Network Models whether samples are downsampled or upsampled but that is not the case when synthetic sampling is used\n",
    "* **\n",
    "* Results are more or less the same using Box Cox transformations and Standaradization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training the model and predicting on unseen dataset:**\n",
    "    * In total 4 modeling techniques are used - \n",
    "        * ** Logistic Regression**\n",
    "        * ** Random Forest**\n",
    "        * ** XGBoost**\n",
    "        * ** Light GBM**\n",
    "* **\n",
    "    * Metrics will be calculated using each modeling technique\n",
    "    * Confusion Matrix numbers are TN, FP, FN, TP\n",
    "* **\n",
    "   * **A joint plot of ROC AUC will be made to see the performance of classifiers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Various ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T00:49:48.807005Z",
     "start_time": "2021-04-03T00:49:48.801987Z"
    }
   },
   "outputs": [],
   "source": [
    "def cal_score(y_test, y_pred): \n",
    "    cm              = confusion_matrix(y_test, y_pred)\n",
    "    prec_scr_macro  = precision_score(y_test, y_pred, average='macro')*100\n",
    "    prec_scr_micro  = precision_score(y_test, y_pred, average='micro')*100\n",
    "    rec_scr_macro   = recall_score(y_test ,y_pred, average='macro')*100\n",
    "    rec_scr_micro   = recall_score(y_test ,y_pred, average='micro')*100\n",
    "    f1_scr_macro    = f1_score(y_test, y_pred, average='macro')*100 \n",
    "    f1_scr_micro    = f1_score(y_test, y_pred, average='micro')*100 \n",
    "    return prec_scr_macro, prec_scr_micro, rec_scr_macro, rec_scr_micro, f1_scr_macro, f1_scr_micro, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T05:20:08.671044Z",
     "start_time": "2021-04-03T05:20:08.664063Z"
    }
   },
   "outputs": [],
   "source": [
    "def metrics_calculation(classifier, training_data, testing_data, training_label, testing_label):\n",
    "    \n",
    "    result = []\n",
    "    cols = ['Mean Accuracy', 'Accuracy deviation', 'Precision Macro', 'Precision Micro', 'Recall Macro','Recall Micro', 'F1 Score Macro', 'F1 Score Micro', 'Confusion Matrix']\n",
    "    \n",
    "    crs_val     = cross_val_score(classifier, training_data, training_label, cv=5)\n",
    "    mean_acc = round(np.mean(crs_val),3)\n",
    "    std_acc  = round(np.std(crs_val),3)\n",
    "    classifier.fit(training_data, training_label)\n",
    "    predictions = classifier.predict(testing_data)\n",
    "    prec_scr_macro, prec_scr_micro, rec_scr_macro, rec_scr_micro, f1_scr_macro, f1_scr_micro, cm = cal_score(testing_label, predictions)\n",
    "    result.extend([mean_acc,std_acc, prec_scr_macro, prec_scr_micro, rec_scr_macro, rec_scr_micro, f1_scr_macro, f1_scr_micro, cm])\n",
    "    series_result = pd.Series(data=result, index=cols)\n",
    "    \n",
    "    return series_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T05:20:16.392159Z",
     "start_time": "2021-04-03T05:20:16.376575Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_dict = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'Logistic Regression' : LogisticRegression(random_state=42),\n",
    "    'Light GBM' : LGBMClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Logistic Regression** : \n",
    "* **\n",
    "    * 1/(1+e^-value)\n",
    "   * **\n",
    "    * p(x) = e^(b0+b1*X)/(1+e^(b0+b1*X))\n",
    "    * ln(p(X)/1-p(X)) = b0+b1*X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Evolution of Tree Based Algorithms and going towards XGboost** -\n",
    "* **\n",
    "* **Decision Tree** : A graphical representation of possible solutions to a decision based on certain conditions.\n",
    "* **\n",
    "* **Bagging** : Bootstrap aggregating or Bagging is a ensemble meta-algorithm combining predictions from multiple decision trees through a majority voting mechanism.\n",
    "* **\n",
    "* **Random Forest** : Bagging based algorithm where only a subset of features are selected at random to build a forest or collection of decision trees.\n",
    "* **\n",
    "* **Boosting** : Models are built sequentially by minizing the errors from previous models while increasing(boosting) influence of high-performing models.\n",
    "* **\n",
    "* **Gradient Boosting** : It employs gradient descent algorithm to minimize errors in sequential models.\n",
    "* **\n",
    "* **XGBoost** : Optimized Gradient Boosting algorithm through parallel processing, tree-pruning,handling missing values and regularization to avoid overfitting/bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Frameowrk of Boosting**:\n",
    "    * F1(x) <- F0(x) + h1(x)\n",
    "    * F2(x) <- F1(x) + h2(x)\n",
    "    * Fm(x) <- Fm-1(x) + hm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Training Box Cox transformed dataset using ML algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T05:26:41.825441Z",
     "start_time": "2021-04-03T05:20:24.768928Z"
    }
   },
   "outputs": [],
   "source": [
    "frame = {}\n",
    "for key in clf_dict:\n",
    "\n",
    "    classifier_result = metrics_calculation(clf_dict[key], os_data_X_tranformed, df_test_transformed, os_data_y, y_test)\n",
    "    frame[key]    = classifier_result\n",
    "        \n",
    "box_cox_smote_df = pd.DataFrame(frame)\n",
    "box_cox_smote_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Training Standaradized dataset using ML algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T05:32:32.508057Z",
     "start_time": "2021-04-03T05:26:49.370801Z"
    }
   },
   "outputs": [],
   "source": [
    "frame_std = {}\n",
    "for key in clf_dict:\n",
    "\n",
    "    classifier_result_std = metrics_calculation(clf_dict[key], X_train_scaled, X_test_scaled, os_data_y, y_test)\n",
    "    frame_std[key]    = classifier_result_std\n",
    "        \n",
    "standardized_smote_df = pd.DataFrame(frame_std)\n",
    "standardized_smote_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T04:38:19.774916Z",
     "start_time": "2021-04-01T04:38:19.760952Z"
    }
   },
   "source": [
    "* ** Training Original dataset with using hyperparameter which is balancing minority class as per the majority class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T05:37:50.086887Z",
     "start_time": "2021-04-03T05:37:50.041008Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_dict_balanced = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, class_weight = {0:1, 1:10}),\n",
    "    'XGBoost': XGBClassifier(random_state=42, scale_pos_weight = 10),\n",
    "    'Logistic Regression' : LogisticRegression(random_state=42, class_weight = {0:1, 1:10}),\n",
    "    'Light GBM' : LGBMClassifier(random_state=42, scale_pos_weight = 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T05:41:02.618787Z",
     "start_time": "2021-04-03T05:37:57.618913Z"
    }
   },
   "outputs": [],
   "source": [
    "frame_balanced = {}\n",
    "for key in clf_dict_balanced:\n",
    "\n",
    "    classifier_result_balanced = metrics_calculation(clf_dict_balanced[key], df_train, df_test, y_train, y_test)\n",
    "    frame_balanced[key]    = classifier_result_balanced\n",
    "        \n",
    "balanced_df = pd.DataFrame(frame_balanced)\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T04:55:59.239488Z",
     "start_time": "2021-04-01T04:55:59.233503Z"
    }
   },
   "source": [
    "* ** XGBoost and Light GBM are performing better in terms of Recall & Light GBM whereas Random Forest in Precision**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Training Scaled oiginal dataset with using hyperparameter which is balancing minority class as per the majority class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T05:44:00.055768Z",
     "start_time": "2021-04-03T05:41:11.304079Z"
    }
   },
   "outputs": [],
   "source": [
    "frame_balanced_scaled= {}\n",
    "for key in clf_dict_balanced:\n",
    "\n",
    "    classifier_result_balanced_scaled = metrics_calculation(clf_dict_balanced[key], df_train_transformed, df_test_transformed, y_train, y_test)\n",
    "    frame_balanced_scaled[key]    = classifier_result_balanced_scaled\n",
    "        \n",
    "balanced_df_scaled = pd.DataFrame(frame_balanced_scaled)\n",
    "balanced_df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:36:12.606845Z",
     "start_time": "2021-04-01T05:36:12.600902Z"
    }
   },
   "source": [
    "* **Logistic Regression metrics are changed while all other model metrics remain same**\n",
    "* Hence, Logistic Regression algorithm requires scaling of features whereas tree based doesn't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Plotting ROC AUC for the ML models for the transformed dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:30:16.196709Z",
     "start_time": "2021-04-02T13:30:16.190725Z"
    }
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "    'label': 'Random Forest',\n",
    "    'model': RandomForestClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "    'label' : 'XGBoost',\n",
    "    'model' : XGBClassifier(random_state=42)\n",
    "    },\n",
    "    {\n",
    "    'label' : 'Logistic Regression',\n",
    "    'model' : LogisticRegression(random_state=42)\n",
    "    },\n",
    "    {\n",
    "    'label' : 'Light GBM',\n",
    "    'model' : LGBMClassifier(random_state=42)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:30:23.142313Z",
     "start_time": "2021-04-02T13:30:23.137142Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_multiplt_rocauc(models,train_X, train_y ,dev_X, dev_y):\n",
    "    \n",
    "    for m in models:\n",
    "        model = m['model']   \n",
    "        model.fit(train_X, train_y)\n",
    "        y_pred = model.predict(dev_X)\n",
    "        pred   = model.predict_proba(dev_X)\n",
    "        pred_new = [i[1] for i in pred]\n",
    "        fpr, tpr, thresholds = roc_curve(dev_y, pred_new)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], roc_auc))\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "    plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()   # Display\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T13:31:42.633971Z",
     "start_time": "2021-04-02T13:30:29.991548Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_multiplt_rocauc(models,os_data_X_tranformed,os_data_y, df_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AUC of all the models are on a similar scale except the XGBoost model which is 0.1 on a higher side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AUC of XGBoost Light GBM are performing beyeer than the others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Key takeaways:**\n",
    "* **\n",
    "    * Tree Based model doesn't require scaling of the features\n",
    "    * Statistical - Logistic Regression requires scaling of the features\n",
    "    * Upsampling of the Minority Class, Downsampling of the Majority Class and SMOTE sampling results are not better as compared to hyperparamater turning on using class weights in the model itself\n",
    "    \n",
    "* ** We will be proceeding with the original dataset without scaling of the featues but using class weights in the model itself**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Feature Importance**\n",
    "    * We will be plotting top features by importance of all the ML algorithms\n",
    "    * we will be plotting Logistic Regression features by correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T10:39:27.725247Z",
     "start_time": "2021-03-30T10:39:27.719264Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, data):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    n_features = data.shape[1]\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), data.columns)\n",
    "    plt.xlabel(\"Feature importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.ylim(-1, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T01:27:30.752522Z",
     "start_time": "2021-03-31T01:27:29.910742Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in clf_dict.keys():\n",
    "    if key =='Logistic Regression':\n",
    "        continue\n",
    "    else:\n",
    "        print('Model is ', key)\n",
    "        plot_feature_importances(clf_dict[key], os_data_X_tranformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In all ML models, ratio of amount owed to the credit limit is the top feature followed by Combined Past Due which is the summation of all dues by a borrower\n",
    "* **\n",
    "* In Light GBM model, others features are also contributing to a greater extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:06:03.521581Z",
     "start_time": "2021-03-31T17:06:00.855668Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(os_data_X_tranformed, os_data_y)\n",
    "\n",
    "coeff_df = pd.DataFrame(os_data_X_tranformed.columns)\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Positive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection using RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-30T16:51:44.235960Z",
     "start_time": "2021-03-30T16:51:44.217012Z"
    }
   },
   "source": [
    "* We will be using RFECV on * **Random Forest** to remove correlated features because they might be providing the same information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T17:06:23.379089Z",
     "start_time": "2021-03-31T17:06:23.372109Z"
    }
   },
   "outputs": [],
   "source": [
    "def RFE(model, training_data, training_label):\n",
    "    rfc = model\n",
    "    rfecv = RFECV(estimator=rfc, step=1, cv=StratifiedKFold(5), scoring='roc_auc')\n",
    "    rfecv.fit(training_data, training_label)\n",
    "\n",
    "    print('Optimal number of features: {}'.format(rfecv.n_features_))\n",
    "    training_data_rfe = training_data.drop(training_data.columns[np.where(rfecv.support_ == False)[0]], axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "    plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n",
    "    plt.show()\n",
    "    \n",
    "    dset = pd.DataFrame()\n",
    "    dset['attr'] = training_data.columns\n",
    "    dset['importance'] = rfecv.estimator_.feature_importances_\n",
    "\n",
    "    dset = dset.sort_values(by='importance', ascending=False)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    plt.barh(y=dset['attr'], width=dset['importance'], color='#1976D2')\n",
    "    plt.title('RFECV - Feature Importances', fontsize=20, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Importance', fontsize=14, labelpad=20)\n",
    "    plt.show()\n",
    "    \n",
    "    return training_data_rfe.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T18:26:10.509502Z",
     "start_time": "2021-03-31T17:06:27.184369Z"
    }
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfecv = RFECV(estimator=rfc, step=1, cv=StratifiedKFold(5), scoring='roc_auc')\n",
    "rfecv.fit(os_data_X_tranformed, os_data_y)\n",
    "\n",
    "print('Optimal number of features: {}'.format(rfecv.n_features_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T18:26:15.522704Z",
     "start_time": "2021-03-31T18:26:15.266484Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T18:26:19.881977Z",
     "start_time": "2021-03-31T18:26:19.834103Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.where(rfecv.support_ == False)[0])\n",
    "\n",
    "os_data_X_tranformed_rfe = os_data_X_tranformed.drop(os_data_X_tranformed.columns[np.where(rfecv.support_ == False)[0]], axis=1)\n",
    "os_data_X_tranformed_rfe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T18:26:24.531068Z",
     "start_time": "2021-03-31T18:26:24.112229Z"
    }
   },
   "outputs": [],
   "source": [
    "dset = pd.DataFrame()\n",
    "dset['attr'] = os_data_X_tranformed.columns\n",
    "dset['importance'] = rfecv.estimator_.feature_importances_\n",
    "\n",
    "dset = dset.sort_values(by='importance', ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "plt.barh(y=dset['attr'], width=dset['importance'], color='#1976D2')\n",
    "plt.title('RFECV - Feature Importances', fontsize=20, fontweight='bold', pad=20)\n",
    "plt.xlabel('Importance', fontsize=14, labelpad=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since our optimal performance is with the complete features and no feature is eliminated, we wil be proceeding with the complete Feature Engineered inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypter Paramter Tuning for the Random Forest, XGBoost, and LightGBM\n",
    "* We will be using GridSearchCV for hyperparamater tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we will be using Metrics such as Precision, Recall, F1 Score on class 1 only\n",
    "* Previously * **macro** and * **micro** were used to give idea how these 2 separate metrics are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T07:20:47.632837Z",
     "start_time": "2021-04-01T07:20:47.618875Z"
    }
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(classifier, training_dataset, test_dataset, training_label, test_label):\n",
    "    \n",
    "    result = []\n",
    "    cols = ['Precision', 'Recall', 'F1 Score', 'Confusion Matrix']\n",
    "    \n",
    "    model_name = {}\n",
    "    class_str = str(classifier)\n",
    "    if 'Random' in class_str:\n",
    "        param_grid={\n",
    "    \"n_estimators\":[27,36,100,200],\n",
    "    \"max_depth\":[5,7,9,15],\n",
    "    \"min_samples_leaf\":[2,4,6,8]\n",
    "        }\n",
    "        model = GridSearchCV(classifier, param_grid = param_grid, cv=StratifiedKFold(5))\n",
    "        model.fit(training_dataset, training_label)\n",
    "        best_est_model = model.best_estimator_\n",
    "        model_name[class_str] = best_est_model\n",
    "        best_est_model.fit(training_dataset, training_label)\n",
    "        y_pred = best_est_model.predict(test_dataset)\n",
    "\n",
    "    elif 'XG' in class_str:\n",
    "        model_name = {}\n",
    "        parameters = {\n",
    "    'n_estimators': [100, 200], \n",
    "  'max_depth': [3, 5, 8], \n",
    "  'gamma' : [0.25,0.5,1],\n",
    "  'reg_alpha': [0.1, 0.25, 0.5],\n",
    "  'reg_lambda': [0.5,1,1.15],\n",
    "  'scale_pos_weight' : [8,10,12,15]\n",
    "    }\n",
    "        model = GridSearchCV(classifier, parameters, scoring=\"neg_log_loss\", cv = StratifiedKFold(5), n_jobs = -1, verbose = 2)\n",
    "        \n",
    "        model.fit(training_dataset, training_label)\n",
    "        best_est_model = model.best_estimator_\n",
    "        model_name[class_str] = best_est_model\n",
    "        best_est_model.fit(training_dataset, training_label)\n",
    "        y_pred = best_est_model.predict(test_dataset)\n",
    "        \n",
    "    else:\n",
    "        model_name = {}\n",
    "        param_grid = {'n_estimators': [100,250,400],\n",
    "          'min_split_gain' : [0.25,0.45,0.7],\n",
    "         'max_depth': [4,7,10],\n",
    "         'subsample': [0.65,0.85],\n",
    "         'colsample_bytree': [0.45,0.65,0.85],\n",
    "         'reg_lambda': [0.5,1,3,5],\n",
    "         'num_leaves' : [30,50,70],\n",
    "         'min_data_in_leaf' : [100,400,600]\n",
    "         }\n",
    "        \n",
    "        model = GridSearchCV(estimator = classifier, param_grid = param_grid, \n",
    "                          cv = StratifiedKFold(5), n_jobs = -1, verbose = 2)\n",
    "        \n",
    "        model.fit(training_dataset, training_label)\n",
    "        best_est_model = model.best_estimator_\n",
    "        model_name[class_str] = best_est_model\n",
    "        best_est_model.fit(training_dataset, training_label)\n",
    "        y_pred = best_est_model.predict(test_dataset)\n",
    "        \n",
    "    prec_scr, rec_scr, f1_scr, cm = cal_score(test_label, y_pred)\n",
    "    result.extend([prec_scr, rec_scr, f1_scr, cm])\n",
    "    series_result = pd.Series(data=result, index=cols)   \n",
    "    \n",
    "    return series_result, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T03:24:17.847727Z",
     "start_time": "2021-04-01T19:23:13.289927Z"
    }
   },
   "outputs": [],
   "source": [
    "frame_hyperparamater = {} ; model_params = []\n",
    "for key in clf_dict:\n",
    "    print(key)\n",
    "    if key == 'Logistic Regression':\n",
    "        continue\n",
    "\n",
    "    hyperparamater_result, model_paramters = hyperparameter_tuning(clf_dict[key], df_train, df_test, y_train, y_test)\n",
    "    frame_hyperparamater[key]    = hyperparamater_result\n",
    "    model_params.append(model_paramters)\n",
    "        \n",
    "tuned_df = pd.DataFrame(frame_hyperparamater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T03:31:30.551602Z",
     "start_time": "2021-04-02T03:31:30.541629Z"
    }
   },
   "outputs": [],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T03:52:35.079514Z",
     "start_time": "2021-04-02T03:52:35.068544Z"
    }
   },
   "outputs": [],
   "source": [
    "clf_dict_balanced = {'Random Forest': RandomForestClassifier(max_depth=15, min_samples_leaf=8, n_estimators=200, random_state=42, class_weight={0:1,1:10}),\n",
    "           'XGBoost': XGBClassifier(gamma=1, max_depth=8, n_estimators=200, random_state=42, reg_alpha=0.5, reg_lambda=1.15, scale_pos_weight=10),\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, class_weight={0:1,1:10}),\n",
    "            'Light GBM': LGBMClassifier(colsample_bytree=0.65, max_depth=4, min_data_in_leaf=400, min_split_gain=0.25, num_leaves=70, random_state=42, reg_lambda=5, subsample=0.65, scale_pos_weight=10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T03:59:07.915079Z",
     "start_time": "2021-04-02T03:52:42.478831Z"
    }
   },
   "outputs": [],
   "source": [
    "frame_balanced = {}\n",
    "for key in clf_dict_balanced:\n",
    "\n",
    "    classifier_result_balanced = metrics_calculation(clf_dict_balanced[key], df_train, df_test, y_train, y_test)\n",
    "    frame_balanced[key]    = classifier_result_balanced\n",
    "        \n",
    "balanced_df = pd.DataFrame(frame_balanced)\n",
    "balanced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:11:59.096123Z",
     "start_time": "2021-04-01T05:11:59.090105Z"
    }
   },
   "source": [
    "* ** Plotting ROC AUC for the ML models for the original dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T04:02:42.257346Z",
     "start_time": "2021-04-02T04:02:42.248370Z"
    }
   },
   "outputs": [],
   "source": [
    "models_balanced = [\n",
    "    {\n",
    "    'label': 'Random Forest',\n",
    "    'model': RandomForestClassifier(max_depth=15, min_samples_leaf=8, n_estimators=200, random_state=42, class_weight={0:1,1:10})\n",
    "    },\n",
    "    {\n",
    "    'label' : 'XGBoost',\n",
    "    'model' : XGBClassifier(gamma=1, max_depth=8, n_estimators=200, random_state=42, reg_alpha=0.5, reg_lambda=1.15, scale_pos_weight=10)\n",
    "    },\n",
    "    {\n",
    "    'label' : 'Logistic Regression',\n",
    "    'model' : LogisticRegression(random_state=42, class_weight={0:1,1:10})\n",
    "    },\n",
    "    {\n",
    "    'label' : 'Light GBM',\n",
    "    'model' : LGBMClassifier(colsample_bytree=0.65, max_depth=4, min_data_in_leaf=400, min_split_gain=0.25, num_leaves=70, random_state=42, reg_lambda=5, subsample=0.65, scale_pos_weight=10)\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-02T04:03:23.108027Z",
     "start_time": "2021-04-02T04:02:49.011997Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_multiplt_rocauc(models,df_train,y_train, df_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will be going with the * **Light GBM** model as it is performing better than the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T11:15:30.772949Z",
     "start_time": "2021-04-09T11:15:29.838448Z"
    }
   },
   "outputs": [],
   "source": [
    "model_lgb = LGBMClassifier(colsample_bytree=0.65, max_depth=4, min_data_in_leaf=400, min_split_gain=0.25, num_leaves=70, random_state=42, reg_lambda=5, subsample=0.65, scale_pos_weight=10)\n",
    "\n",
    "model_lgb.fit(df_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-09T11:18:53.242113Z",
     "start_time": "2021-04-09T11:18:53.239073Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred                 = model_lgb.predict(df_test)\n",
    "df_test['predictions'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:15:02.132193Z",
     "start_time": "2021-04-10T02:15:02.129201Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_model_lgb = lambda x: model_lgb.predict_proba(x).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance and Model Explaination using SHAP and LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Why Interpretability in Machine Learning model is needed:**\n",
    "    * **\n",
    "    * Fairness - We might be interested in trends of the feature wrt to target variable.\n",
    "    * Checking causality of features & Debugging models - We might be interested in what pattern among the features model is capturing.\n",
    "    * Regulations -Sometimes regulatory bodies need answers taken on model inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explaining the model performance using SHAP so that black box model can become transparent one.\n",
    "* **We will be looking at the model transparency using SHAP at global level and LIME at a local level**\n",
    "* **\n",
    "* The collective SHAP values can show how much each predictor contributes, either positively or negatively, to the target variable. This is like the variable importance plot but it is able to show the positive or negative relationship for each variable with the target .\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **SHAP(SHapely Additive exPlainations)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:48:35.074888Z",
     "start_time": "2021-04-01T05:48:35.071895Z"
    }
   },
   "source": [
    "#### Variable Importance Plot — Global Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:41:09.326309Z",
     "start_time": "2021-04-01T05:40:48.244783Z"
    }
   },
   "outputs": [],
   "source": [
    "X_importance = df_test\n",
    "\n",
    "# Explain model predictions using shap library:\n",
    "explainer = shap.TreeExplainer(model_lgb)\n",
    "shap_values = explainer.shap_values(X_importance)\n",
    "# Plot summary_plot\n",
    "shap.summary_plot(shap_values[1], X_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature importance: Variables are ranked in descending order.\n",
    "* **\n",
    "* Impact: The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n",
    "* **\n",
    "* Original value: Color shows whether that variable is high (in red) or low (in blue) for that observation.\n",
    "* **\n",
    "* Correlation: A high level of the “RevolvingUtilizationOfUnsecuredLines” and \"Debt Ratio\" has a high and positive impact on the defaulting. The “high” comes from the red color, and the “positive” impact is shown on the X-axis. Similarly, we will say the “age” is equally correlated with the target variable and non-target variable which might not give any good performance.\n",
    "* \"Monthly Income\" effect is comparitvely low toward prediting values and even the effect is alos low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SHAP Dependence Plot — Global Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The partial dependence plot shows the marginal effect one or two features have on the predicted outcome of a machine learning model. It tells whether the relationship between the target and a feature is linear, monotonic or more complex. \n",
    "* The function automatically includes another variable that your chosen variable interacts most with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:52:42.866561Z",
     "start_time": "2021-04-01T05:52:41.699682Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"RevolvingUtilizationOfUnsecuredLines\", shap_values[1], X_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following plot shows there is an very little linear and a bit of positive trend between “RevolvingUtilizationOfUnsecuredLines” and the target variable, and “RevolvingUtilizationOfUnsecuredLines” does not interact with “CombinedPastDue”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:56:31.960695Z",
     "start_time": "2021-04-01T05:56:30.924476Z"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"age\", shap_values[1], X_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T05:57:21.855838Z",
     "start_time": "2021-04-01T05:57:21.850850Z"
    }
   },
   "source": [
    "* The following plot shows there is an good linear and a negative trend between “age” and the target variable, and “age” interacts with “CombinedPastDue” not so much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIME(Local Interpretable Model agnostic Explanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** If we want to understand how a single prediction was made for a given observation. This is where we use the LIME technique which stands for local interpretable model agnostic explanations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generating explainations using LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:15:07.778210Z",
     "start_time": "2021-04-10T02:15:07.673489Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating the explainer function\n",
    "explainer = LimeTabularExplainer(df_test.values, mode=\"classification\", feature_names=df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:22:04.490545Z",
     "start_time": "2021-04-10T02:22:03.748567Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(df_test.iloc[3], \n",
    "     predict_model_lgb, num_features=10)\n",
    "exp.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Green/Red color: features that have positive correlations with the target are shown in green, otherwise red.\n",
    "* No of times 30-59, 59-90 days and 90 days past due having no values means not missed any due date showing veryr high negative correlation with the target\n",
    "* age having less values lower than 41 relatively show positive corrrelation with the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:15:09.457102Z",
     "start_time": "2021-04-10T02:15:09.079114Z"
    }
   },
   "outputs": [],
   "source": [
    "# storing a new observation\n",
    "i = 37\n",
    "X_observation = df_test.iloc[[i], :]\n",
    "\n",
    "# explanation using the random forest model\n",
    "explanation = explainer.explain_instance(X_observation.values[0], predict_model_lgb)\n",
    "explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "print(explanation.score)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each feature’s contribution to this prediction is shown in the right bar plot. Orange signifies the positive impact and blue signifies the negative impact of that feature on the target. For example, CombinedPastDue has a positive impact on defaulting for this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-10T02:15:10.693793Z",
     "start_time": "2021-04-10T02:15:10.359685Z"
    }
   },
   "outputs": [],
   "source": [
    "# storing a new observation\n",
    "i = 25\n",
    "X_observation = df_test.iloc[[i], :]\n",
    "\n",
    "# explanation using the random forest model\n",
    "explanation = explainer.explain_instance(X_observation.values[0], predict_model_lgb)\n",
    "explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "print(explanation.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-03T05:49:55.815273Z",
     "start_time": "2021-04-03T05:49:55.809289Z"
    }
   },
   "source": [
    "* Each feature’s contribution to this prediction is shown in the right bar plot. Orange signifies the positive impact and blue signifies the negative impact of that feature on the target. For example, CombinedPastDue has a positive impact on non delinquency for this row which has value 0 and Monthly Income feature having value greater than 73000 contributing towards non delinquency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f86def2596adbac183d226219f0d6fb522edfd9e85524a83a5293ed30ef4c26e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
